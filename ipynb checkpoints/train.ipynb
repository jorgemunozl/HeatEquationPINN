{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bdfd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Residual:  tensor(8.4420e-12, grad_fn=<MeanBackward0>)\n",
      "Loss Ic:  tensor(11.0278, grad_fn=<MeanBackward0>)\n",
      "Loss Bc:  tensor(4.4919e-11, grad_fn=<AddBackward0>)\n",
      "General Loss:  tensor(66.1669, grad_fn=<AddBackward0>)\n",
      "Loss Residual:  tensor(2.6021e-06, grad_fn=<MeanBackward0>)\n",
      "Loss Ic:  tensor(0.0498, grad_fn=<MeanBackward0>)\n",
      "Loss Bc:  tensor(1.9320e-05, grad_fn=<AddBackward0>)\n",
      "General Loss:  tensor(0.2991, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    235\u001b[39m         model = train_pinn()\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(flag)\u001b[39m\n\u001b[32m    233\u001b[39m     plotter.init_con()\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     model = \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mtrain_pinn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    175\u001b[39m optimizer.zero_grad()\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Residual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m residual = \u001b[43mcompute_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_col_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_col_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m loss_residual = torch.mean(residual**\u001b[32m2\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Initial\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcompute_residual\u001b[39m\u001b[34m(model, x, t)\u001b[39m\n\u001b[32m     17\u001b[39m d_t = torch.autograd.grad(\n\u001b[32m     18\u001b[39m     u, t, grad_outputs=ones, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     19\u001b[39m     )[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m d_x = torch.autograd.grad(\n\u001b[32m     21\u001b[39m     u, x, grad_outputs=ones, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     22\u001b[39m )[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m d_xx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m d_t - pinnConfig().alpha * d_xx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/githubProjects/PINNFuture/.env/lib/python3.12/site-packages/torch/autograd/__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/githubProjects/PINNFuture/.env/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "def initial_condition(x):\n",
    "    return 10*(x-x**2)**2 + 3\n",
    "\n",
    "\n",
    "def compute_residual(model, x, t):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u = model(x, t)\n",
    "    ones = torch.ones_like(u)\n",
    "    d_t = torch.autograd.grad(\n",
    "        u, t, grad_outputs=ones, create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "    d_x = torch.autograd.grad(\n",
    "        u, x, grad_outputs=ones, create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "    d_xx = torch.autograd.grad(\n",
    "        d_x, x, grad_outputs=torch.ones_like(d_x), create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    return d_t - pinnConfig().alpha * d_xx\n",
    "\n",
    "class netConfig(BaseModel):\n",
    "    save_path: str = Field(\n",
    "        default=\"parameters_new_ic.pth\",\n",
    "        description=\"Parameter's path\"\n",
    "    )\n",
    "    neuron_inputs: int = Field(\n",
    "        default=2,\n",
    "        description='Number of neurons'\n",
    "    )\n",
    "    neuron_hidden: int = Field(\n",
    "        default=128,\n",
    "        description='Number of neurons'\n",
    "    )\n",
    "    hidden_layers_numbers: int = Field(\n",
    "        default=8,\n",
    "    )\n",
    "    neuron_outputs: int = Field(\n",
    "        default=1\n",
    "    )\n",
    "    epochs: int = Field(\n",
    "        default=7000,\n",
    "        description='Number of times that the parameter actualize'\n",
    "    )\n",
    "    lr: float = Field(\n",
    "        default=1e-3,\n",
    "        description='Learning rate'\n",
    "    )\n",
    "\n",
    "\n",
    "class plotConfig(BaseModel):\n",
    "    sample: int = Field(\n",
    "        default=100,\n",
    "        description='Number o'\n",
    "    )\n",
    "    snapshot_step: int = Field(\n",
    "        default=10,\n",
    "        description=\"\"\n",
    "    )\n",
    "    snap_x: int = Field(\n",
    "        default=1000,\n",
    "        description=\"\"\n",
    "    )\n",
    "    snap_t: int = Field(\n",
    "        default=100,\n",
    "        description=\"\"\n",
    "    )\n",
    "    frames_snap: int = Field(\n",
    "        default=100,\n",
    "        description=\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class pinnConfig(BaseModel):\n",
    "    alpha: float = Field(\n",
    "        default=0.1,\n",
    "        description=\"Important for the PDE\"\n",
    "    )\n",
    "\n",
    "    num_collocation_res: int = Field(\n",
    "        default=1000,\n",
    "        description=\"\"\n",
    "    )\n",
    "    num_collocation_ic: int = Field(\n",
    "        default=500,\n",
    "        description=''\n",
    "    )\n",
    "    num_collocation_bc: int = Field(\n",
    "        default=600,\n",
    "        description=''\n",
    "    )\n",
    "    lambda_residual: float = Field(\n",
    "        default=10.0,\n",
    "        description=''\n",
    "    )\n",
    "    lambda_ic: float = Field(\n",
    "        default=6.0,\n",
    "        description=''\n",
    "    )\n",
    "    lambda_bc: float = Field(\n",
    "        default=5.0,\n",
    "        description=''\n",
    "    )\n",
    "    error_x_sample: int = Field(\n",
    "        default=1000,\n",
    "        description=''\n",
    "    )\n",
    "    error_t_sample: int = Field(\n",
    "        default=100,\n",
    "        description=''\n",
    "    )\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer = [nn.Linear(netConfig().neuron_inputs,\n",
    "                           netConfig().neuron_hidden), nn.GELU()]\n",
    "        for i in range(netConfig().hidden_layers_numbers):\n",
    "            layer += [nn.Linear(netConfig().neuron_hidden,\n",
    "                                netConfig().neuron_hidden), nn.GELU()]\n",
    "        layer += [nn.Linear(netConfig().neuron_hidden,\n",
    "                            netConfig().neuron_outputs), nn.GELU()]\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = torch.cat([x, t], dim=1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "def train_pinn():\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=netConfig().lr)\n",
    "    num_collocation_res = pinnConfig().num_collocation_res\n",
    "    num_collocation_ic = pinnConfig().num_collocation_ic\n",
    "    num_collocation_bc = pinnConfig().num_collocation_bc\n",
    "    lambda_residual = pinnConfig().lambda_residual\n",
    "    lambda_ic = pinnConfig().lambda_ic\n",
    "    lambda_bc = pinnConfig().lambda_bc\n",
    "\n",
    "    # Residual Collocation\n",
    "    x_col_res = torch.rand(num_collocation_res, 1)\n",
    "    t_col_res = torch.rand(num_collocation_res, 1)\n",
    "\n",
    "    # Initial Condition Collocation\n",
    "    x_col_ic = torch.rand(num_collocation_ic, 1)\n",
    "    t_col_ic = torch.zeros((num_collocation_ic, 1))\n",
    "\n",
    "    # Boundary Condition Collocation\n",
    "    t_x_bc = torch.rand(num_collocation_bc, 1)\n",
    "    x_bc = torch.zeros((num_collocation_bc, 1), requires_grad=True)\n",
    "    t_l_bc = torch.rand(num_collocation_bc, 1)\n",
    "    l_bc = torch.ones((num_collocation_bc, 1), requires_grad=True)\n",
    "\n",
    "    # Neumann\n",
    "    ux_0_bc = torch.zeros((num_collocation_bc, 1))\n",
    "    ux_1_bc = torch.zeros((num_collocation_bc, 1))\n",
    "\n",
    "    # Snapshot values\n",
    "\n",
    "    snapshots = torch.zeros((plotConfig().snap_x,\n",
    "                             plotConfig().snap_t,\n",
    "                             plotConfig().frames_snap))\n",
    "\n",
    "    for _ in range(netConfig().epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Residual\n",
    "        residual = compute_residual(model, x_col_res, t_col_res)\n",
    "        loss_residual = torch.mean(residual**2)\n",
    "\n",
    "        # Initial\n",
    "        model_ic = model(x_col_ic, t_col_ic)\n",
    "        loss_ic = torch.mean((model_ic-initial_condition(x_col_ic))**2)\n",
    "\n",
    "        # Boundary\n",
    "        u_0_bc = model(x_bc, t_x_bc)\n",
    "        du_0_bc = torch.autograd.grad(\n",
    "            u_0_bc, x_bc, grad_outputs=torch.ones_like(u_0_bc),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        u_l_bc = model(l_bc, t_l_bc)\n",
    "        du_l_bc = torch.autograd.grad(\n",
    "            u_l_bc, l_bc, grad_outputs=torch.ones_like(u_l_bc),\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        loss_0_bc = torch.mean((du_0_bc-ux_0_bc)**2)\n",
    "        loss_1_bc = torch.mean((du_l_bc-ux_1_bc)**2)\n",
    "        loss_b = (loss_0_bc + loss_1_bc)\n",
    "        loss = lambda_residual*loss_residual+lambda_ic*loss_ic+lambda_bc*loss_b\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if _ % 100 == 0:\n",
    "            print(\"Loss Residual: \", loss_residual)\n",
    "            print(\"Loss Ic: \", loss_ic)\n",
    "            print(\"Loss Bc: \", loss_b)\n",
    "            print(\"General Loss: \", loss)\n",
    "\n",
    "        \"\"\"\n",
    "        plotter = plots()\n",
    "        if _ % plotConfig().snapshot_step == 0:\n",
    "            if _ == netConfig().epochs-1:\n",
    "                plotter.animate_snapshot(model, snapshots, _, True)\n",
    "            else:\n",
    "                plotter.animate_snapshot(model, snapshots, _, False)\n",
    "        \"\"\"\n",
    "\n",
    "    save_path = netConfig().save_path\n",
    "    torch.save(\n",
    "            {'model_state_dict': model.state_dict()}, save_path\n",
    "        )\n",
    "    return model, snapshots\n",
    "\n",
    "\n",
    "def main(flag: bool):\n",
    "    if flag:\n",
    "        model = NeuralNetwork()\n",
    "        loaded = torch.load(netConfig().save_path)\n",
    "        model.load_state_dict(loaded[\"model_state_dict\"])\n",
    "        model.eval()\n",
    "        plotter = plots()\n",
    "        plotter.init_con()\n",
    "    else:\n",
    "        model = train_pinn()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
